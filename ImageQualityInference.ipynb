{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Quality Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--archives tf-env-2.4.zip#venv pyspark-shell'\n",
    "os.environ['PYSPARK_PYTHON'] = 'venv/bin/python'\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('Image pipeline')\n",
    "    .master('yarn')\n",
    "    .config(\n",
    "        'spark.driver.extraJavaOptions',\n",
    "        ' '.join('-D{}={}'.format(k, v) for k, v in {\n",
    "            'http.proxyHost': 'webproxy.eqiad.wmnet',\n",
    "            'http.proxyPort': '8080',\n",
    "            'https.proxyHost': 'webproxy.eqiad.wmnet',\n",
    "            'https.proxyPort': '8080',\n",
    "        }.items()))\n",
    "    .config('spark.jars.packages', 'com.linkedin.sparktfrecord:spark-tfrecord_2.11:0.2.4')\n",
    "    .config(\"spark.driver.memory\", \"4g\") \n",
    "    .config('spark.dynamicAllocation.maxExecutors', 128) \n",
    "    .config(\"spark.executor.memory\", \"8g\") \n",
    "    .config(\"spark.executor.cores\", 4) \n",
    "    .config(\"spark.sql.shuffle.partitions\", 512)\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 1024)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType, IntegerType\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('/home/aikochou/ImagePipeline/image_quality_model')\n",
    "model_json = model.to_json()\n",
    "bc_model_weights = sc.broadcast(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data into Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.schema('image_file_name string, image_bytes binary, label int').format(\"tfrecord\")\n",
    "        .option(\"recordType\", \"Example\").load('image.tfrecords'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|     image_file_name|         image_bytes|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|Star_Magnolia_Mag...|[FF D8 FF E0 00 1...|    1|\n",
      "|Heppenheim_BW_201...|[FF D8 FF E0 00 1...|    1|\n",
      "|Lloyd's_Building_...|[FF D8 FF E0 00 1...|    1|\n",
      "|B-Spandau_Okt12_R...|[FF D8 FF E0 00 1...|    1|\n",
      "|Rösrath_Germany_W...|[FF D8 FF E0 00 1...|    1|\n",
      "|Marmoutier_eglise...|[FF D8 FF E0 00 1...|    1|\n",
      "|15-11-25-Maribor-...|[FF D8 FF E0 00 1...|    1|\n",
      "|Mägiste_peatuskoh...|[FF D8 FF E0 00 1...|    1|\n",
      "|USMC-111209-M-XR0...|[FF D8 FF E0 00 1...|    0|\n",
      "|Eglise_Saint-Séve...|[FF D8 FF E0 00 1...|    0|\n",
      "|Церковь_-_panoram...|[FF D8 FF E0 00 1...|    0|\n",
      "|Exterieur_zuidgev...|[FF D8 FF E0 00 1...|    0|\n",
      "|Поле_-_panoramio_...|[FF D8 FF E0 00 1...|    0|\n",
      "|Père-Lachaise_-_D...|[FF D8 FF E0 00 1...|    1|\n",
      "|Rosa_'Aachener_Do...|[FF D8 FF E0 00 1...|    0|\n",
      "|Window_of_the_Sai...|[FF D8 FF E0 00 1...|    1|\n",
      "|St_Petka_church_-...|[FF D8 FF E0 00 1...|    1|\n",
      "|Ribadavia_-_Galiz...|[FF D8 FF E0 00 1...|    1|\n",
      "|1991_Volkswagen_T...|[FF D8 FF E0 00 1...|    0|\n",
      "|Beach_(4706533028...|[FF D8 FF E0 00 1...|    0|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model inference via pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 180\n",
    "batch_size = 64\n",
    "\n",
    "@F.pandas_udf(ArrayType(FloatType()))\n",
    "def process_image(image_bytes):\n",
    "    ret = []\n",
    "    for image in image_bytes:\n",
    "        im = Image.open(BytesIO(image))\n",
    "        im = im.resize([image_size,image_size])\n",
    "        image_data = [float(i) for i in np.asarray(im, dtype='float32').flatten()]\n",
    "        ret.append(image_data)\n",
    "    return pd.Series(ret)\n",
    "\n",
    "def parse_image(image_data):\n",
    "    image = tf.image.convert_image_dtype(image_data, dtype=tf.float32) * (2. / 255) - 1 # normalization\n",
    "    image = tf.reshape(image,[image_size,image_size,3])\n",
    "    return image\n",
    "\n",
    "@F.pandas_udf(ArrayType(FloatType()))\n",
    "def predict_batch_udf(image_batch):\n",
    "    model = model_from_json(model_json) # load the model graph \n",
    "    model.set_weights(bc_model_weights.value) # set the weights from the broadcasted variables\n",
    "    images = np.vstack(image_batch)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=8).prefetch(5000).batch(batch_size)\n",
    "    preds = model.predict(dataset)\n",
    "    return pd.Series(list(preds))\n",
    "\n",
    "predict_label_udf = F.udf(lambda y: \"Quality\" if y[0] > 0.5 else \"Random\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .withColumn('image_arr', process_image(F.col('image_bytes')))\n",
    " .withColumn('prediction', predict_batch_udf(F.col('image_arr')))\n",
    " .withColumn('pred_label', predict_label_udf(F.col('prediction')))\n",
    " .select('image_file_name', 'label', 'pred_label')\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .parquet('output.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and check the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = spark.read.load('output.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|     image_file_name|label|pred_label|\n",
      "+--------------------+-----+----------+\n",
      "|Star_Magnolia_Mag...|    1|   Quality|\n",
      "|Heppenheim_BW_201...|    1|    Random|\n",
      "|Lloyd's_Building_...|    1|    Random|\n",
      "|B-Spandau_Okt12_R...|    1|   Quality|\n",
      "|Rösrath_Germany_W...|    1|   Quality|\n",
      "|Marmoutier_eglise...|    1|   Quality|\n",
      "|15-11-25-Maribor-...|    1|   Quality|\n",
      "|Mägiste_peatuskoh...|    1|    Random|\n",
      "|USMC-111209-M-XR0...|    0|    Random|\n",
      "|Eglise_Saint-Séve...|    0|   Quality|\n",
      "|Церковь_-_panoram...|    0|    Random|\n",
      "|Exterieur_zuidgev...|    0|    Random|\n",
      "|Поле_-_panoramio_...|    0|    Random|\n",
      "|Père-Lachaise_-_D...|    1|   Quality|\n",
      "|Rosa_'Aachener_Do...|    0|   Quality|\n",
      "|Window_of_the_Sai...|    1|   Quality|\n",
      "|St_Petka_church_-...|    1|   Quality|\n",
      "|Ribadavia_-_Galiz...|    1|    Random|\n",
      "|1991_Volkswagen_T...|    0|    Random|\n",
      "|Beach_(4706533028...|    0|    Random|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
